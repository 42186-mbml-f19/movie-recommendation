{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pystan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pystan_utils\n",
    "import os\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "#dataset = 'ml-20m' #big\n",
    "dataset = 'ml-latest-small' #small\n",
    "ratings = pd.read_csv(os.path.join(dataset,'ratings.csv'))\n",
    "# HACK -- small movies.csv is apararently missing movies from small ratings.csv\n",
    "movies = pd.read_csv(os.path.join('ml-20m','movies.csv')) \n",
    "#Create like column\n",
    "ratings['like'] = (ratings.rating > 3.5 )+ 0\n",
    "\n",
    "unique_movies = ratings['movieId'].unique()\n",
    "\n",
    "#movieId is not sequential\n",
    "movie_dict = {movieId: i for i, movieId in enumerate(unique_movies)}\n",
    "ratings['movieId'] = ratings['movieId'].apply(lambda movieId: movie_dict[movieId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now instead of multiclass classification based on stars, turn problem into binary classification by defining 'like' for all movies rated above 3.5 stars, and 'not-like' for all movies below 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = ratings[ratings['userId'] == 1]\n",
    "user['like'] = user.rating > 3.5\n",
    "like = user['like'] + 0\n",
    "N = len(like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user split sample (without replacement) 90% of data as training data and the remaining 10% as validation data. As some movies might never be sampled in the training set, remove those films from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.1\n",
    "#For sample randomly the validation set (note some movies might never be sampled)\n",
    "val_set = ratings.groupby('userId').apply(lambda g: g.sample(frac=val_size,random_state=seed))\n",
    "val_set.index =  val_set.index.droplevel()\n",
    "#train set is compliment of val_set\n",
    "train_set = ratings[~ratings.isin(val_set).all(1)]\n",
    "#Possibly remove movies from validation set that was never sampled in the dataset\n",
    "val_set = val_set[val_set.movieId.isin(train_set.movieId)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that not too many samples was removed. Fraction of the removed data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(ratings) - (len(train_set)+len(val_set)))/len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the movieIds does not necesarrily correspond to integer indices, make new ids such that they can be used as indices in stan vectors/matrixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys = train_set.movieId.unique()\n",
    "indices = range(1,len(unique_keys)+1)\n",
    "movie_id_dict = dict(zip(unique_keys, indices ))\n",
    "id_movie_dict = dict(zip(indices, unique_keys))\n",
    "train_set['movieIdNoHoles'] = train_set['movieId'].apply(lambda movie_id: movie_id_dict[movie_id])\n",
    "val_set['movieIdNoHoles'] = val_set['movieId'].apply(lambda movie_id: movie_id_dict[movie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One person classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Stan model\n",
    "model_definition = \"\"\"\n",
    "\n",
    "data {\n",
    "    int<lower=0> N;             // number of data items\n",
    "    int likes[N];\n",
    "    \n",
    "}\n",
    "parameters {\n",
    "    vector[N] trait;\n",
    "    real preference;\n",
    "} \n",
    "model {\n",
    "    vector[N] affinity;\n",
    "    vector[N] noisy_affinity;\n",
    "    \n",
    "    preference ~ normal(0,10);\n",
    "    for (n in 1:N){\n",
    "        trait[n] ~ normal(0,10);\n",
    "        affinity[n] = trait[n]*preference;\n",
    "        likes[n] ~ bernoulli_logit(affinity[n]);\n",
    "    }\n",
    "\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'N': N, 'likes': like}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Stan model object\n",
    "sm = pystan.StanModel(model_code=model_definition)\n",
    "fit = sm.sampling(data=data, iter=10000, algorithm=\"NUTS\", chains=1, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit.traceplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Stan model\n",
    "model_definition = \"\"\"\n",
    "\n",
    "data {\n",
    "    int<lower=0> N;             // number of data items\n",
    "    int likes[N];\n",
    "    int num_traits;\n",
    "    \n",
    "}\n",
    "parameters {\n",
    "    matrix[N,num_traits] trait;\n",
    "    vector[num_traits] preference;\n",
    "} \n",
    "model {\n",
    "    //matrix[N, num_traits] trait_affinity ;\n",
    "    vector[N] affinity ;\n",
    "    \n",
    "    preference ~ normal(0,10);\n",
    "    for (n in 1:N){\n",
    "        real tmp = 0;\n",
    "        for (t in 1:num_traits){\n",
    "            trait[n,t] ~ normal(0,10);\n",
    "            tmp += trait[n,t]*preference[t];\n",
    "        \n",
    "        }\n",
    "        affinity[n] = tmp;\n",
    "        likes[n] ~ bernoulli_logit(affinity[n]);\n",
    "        \n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'N': N, 'likes': like, 'num_traits':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Stan model object\n",
    "sm = pystan.StanModel(model_code=model_definition)\n",
    "fit = sm.sampling(data=data, iter=10000, algorithm=\"NUTS\", chains=1, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trait_mean =np.mean(fit['trait'],0)\n",
    "np.subtract(trait_mean[:,0],trait_mean[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for (userId, movieId) in [(u1,m1),(u2,m2),...,(uN,mN)]\n",
    "    affinity = 0;\n",
    "    for (t in 1:num_traits){\n",
    "        traitAffinity = trait[movieId, t] * preference[userId, t];\n",
    "        affinity += traitAffinity\n",
    "    generate prediction such that prediction ~ bernoulli_logit(affinity);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably summing the trait affinities and using the affinity as logit is not the way to discrimitate between likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figs/PGM.png \"Title\")\n",
    "*PGM of model. We use the notation of http://www.mbmlbook.com that specifices the pgm as a bipartite graph where the squares explicitely denotes the distribution* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_movies = len(ratings.movieId.unique())\n",
    "#num_users = len(ratings.userId.unique())\n",
    "#missing_userId = []\n",
    "#missing_movieId = []\n",
    "#Find missing values\n",
    "#all_users = ratings.userId\n",
    "#for movie_id in ratings.movieId.unique():\n",
    "#    missing_users = set(all_users).difference(set(ratings[ratings['movieId']==movie_id].userId))\n",
    "#    for i in missing_users:\n",
    "#        missing_userId.append(i)\n",
    "#        missing_movieId.append(movie_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = \"\"\" data {\n",
    "    int num_movies;             // number of data items\n",
    "    int num_traits;\n",
    "    int num_users;  \n",
    "    \n",
    "    int num_likes;\n",
    "\n",
    "    \n",
    "    int likes_obs[num_likes];\n",
    "    int userId_obs[num_likes];\n",
    "    int movieId_obs[num_likes];\n",
    "    \n",
    "    int num_missing;\n",
    "    int userId_missing [num_missing];\n",
    "    int movieId_missing [num_missing];\n",
    "    \n",
    "}\n",
    "parameters {\n",
    "    matrix[num_movies,num_traits] trait;\n",
    "    matrix[num_users ,num_traits] preference;\n",
    "    \n",
    "} \n",
    "\n",
    "model {\n",
    "    real affinity;\n",
    "\n",
    "    for (n in 1:num_likes){\n",
    "        affinity = 0;\n",
    "        for (t in 1:num_traits){\n",
    "            preference[userId_obs[n], t] ~ normal(0,10);\n",
    "            trait[movieId_obs[n], t] ~ normal(0,10);\n",
    "            \n",
    "            affinity += trait[movieId_obs[n], t]*preference[userId_obs[n], t];\n",
    "            \n",
    "        }\n",
    "        \n",
    "        likes_obs[n] ~ bernoulli_logit(affinity);\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    int predictions[num_missing];\n",
    "    \n",
    "    for(i in 1:num_missing){\n",
    "        real affinity = 0;\n",
    "        for (t in 1:num_traits){\n",
    "            affinity += trait[movieId_missing[i], t] * preference[userId_missing[i], t];\n",
    "       }\n",
    "        predictions[i] = bernoulli_logit_rng(affinity);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' data = {'num_movies': num_movies,\n",
    "       'likes_obs': ratings['like'], \n",
    "        'num_traits':2, \n",
    "        'num_users':num_users, \n",
    "        'num_likes':len(ratings), \n",
    "        'userId_obs': ratings['userId'],\n",
    "        'movieId_obs':ratings['movieId']+1,\n",
    "        'num_missing': len(missing_userId),\n",
    "        'userId_missing': missing_userId,\n",
    "        'movieId_missing': missing_movieId\n",
    "       }\n",
    "'''\n",
    "num_movies = len(train_set.movieIdNoHoles.unique())\n",
    "num_users = len(train_set.userId.unique())\n",
    "data = {'num_movies': num_movies,\n",
    "        'likes_obs': train_set['like'], \n",
    "        'num_traits': 2, \n",
    "        'num_users': num_users, \n",
    "        'num_likes': len(train_set), \n",
    "        'userId_obs': train_set['userId'],\n",
    "        'movieId_obs': train_set['movieIdNoHoles'],\n",
    "        'num_missing': len(val_set),\n",
    "        'userId_missing': val_set['userId'],\n",
    "        'movieId_missing': val_set['movieIdNoHoles']\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Stan model object\n",
    "sm = pystan.StanModel(model_code=model_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#sampling takes forever here, but VB seems to work really well\n",
    "#fit = sm.sampling(data=data, iter=100, algorithm=\"NUTS\", chains=1, seed=seed, verbose=True)\n",
    "fit2 = sm.vb(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show preferences\n",
    "Here the latent traits and preferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences=pystan_utils.vb_extract_variable(fit2, 'preference', var_type='matrix', dims=[num_users,2])\n",
    "traits=pystan_utils.vb_extract_variable(fit2, 'trait', var_type='matrix', dims=[num_movies,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preferences[:,0], preferences[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(traits[:,0], traits[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noted that the plot is a little bit misleading as there are no guarentee that the traits are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some extreme values of trait0. We would expect to see that the trait is discriminating between films using a latent trait of the film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_extreme = 10\n",
    "sorted_trait_0_ids = np.argsort(traits[:,0])\n",
    "lowest_ids = sorted_trait_0_ids[:n_extreme]\n",
    "highest_ids = sorted_trait_0_ids[-n_extreme:]\n",
    "traits[lowest_ids,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits[highest_ids,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_movie_ids = [ id_movie_dict[lowest_id] for lowest_id in lowest_ids]\n",
    "lowest_movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies.movieId.isin(lowest_movie_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_movie_ids = [ id_movie_dict[highest_id] for highest_id in highest_ids]\n",
    "movies[movies.movieId.isin(highest_movie_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspection of the low/high scoring, we cannot really see any latent trait that is used for discriminating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate precision of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pystan_utils.vb_extract_variable(fit2, 'predictions', var_type='vector', dims=[len(val_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = val_set['like']\n",
    "1 - sum(abs(predictions - true_labels))/len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presicion is low close to random and therefore at this time we cannot say that our model is actually precictiong something usefull."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
