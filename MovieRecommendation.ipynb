{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pystan_utils\n",
    "import os\n",
    "import seaborn as sn\n",
    "from  movie_recommendation_aux import *\n",
    "from baselines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "#dataset = 'ml-20m' #big\n",
    "dataset = 'ml-latest-small' #small\n",
    "ratings = pd.read_csv(os.path.join(dataset,'ratings.csv'))\n",
    "# HACK -- small movies.csv is apararently missing movies from small ratings.csv\n",
    "movies = pd.read_csv(os.path.join('ml-20m','movies.csv')) \n",
    "#Create like column\n",
    "ratings['like'] = (ratings.rating >= 3.0 )+ 0\n",
    "\n",
    "#Convert ratings from half step stars, to 1-10 stars\n",
    "ratings_dict = {j:i+1 for i,j in enumerate(sorted(ratings['rating'].unique()))}\n",
    "ratings['rating'] =  ratings['rating'].apply(lambda rating: ratings_dict[rating])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now instead of multiclass classification based on stars, turn problem into binary classification by defining 'like' for all movies rated 3.0 stars or above, and 'not-like' for all movies below 3.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user split sample (without replacement) 90% of data as training data and the remaining 10% as validation data. As some movies might never be sampled in the training set, remove those films from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.3\n",
    "#For sample randomly the validation set (note some movies might never be sampled)\n",
    "val_set = ratings.groupby('userId').apply(lambda g: g.sample(frac=val_size,random_state=seed))\n",
    "val_set.index =  val_set.index.droplevel()\n",
    "#train set is compliment of val_set\n",
    "train_set = ratings[~ratings.isin(val_set).all(1)]\n",
    "#Possibly remove movies from validation set that was never sampled in the dataset\n",
    "val_set = val_set[val_set.movieId.isin(train_set.movieId)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that not too many samples was removed. Fraction of the removed data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(ratings) - (len(train_set)+len(val_set)))/len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the movieIds does not necesarrily correspond to integer indices, make new ids such that they can be used as indices in stan vectors/matrixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys = train_set.movieId.unique()\n",
    "indices = range(1,len(unique_keys)+1)\n",
    "movie_id_dict = dict(zip(unique_keys, indices ))\n",
    "id_movie_dict = dict(zip(indices, unique_keys))\n",
    "train_set['movieIdNoHoles'] = train_set['movieId'].apply(lambda movie_id: movie_id_dict[movie_id])\n",
    "val_set['movieIdNoHoles'] = val_set['movieId'].apply(lambda movie_id: movie_id_dict[movie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for (userId, movieId) in [(u1,m1),(u2,m2),...,(uN,mN)]\n",
    "    affinity = 0;\n",
    "    for (t in 1:num_traits){\n",
    "        traitAffinity = trait[movieId, t] * preference[userId, t];\n",
    "        affinity += traitAffinity\n",
    "    generate prediction such that prediction ~ bernoulli_logit(affinity);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably summing the trait affinities and using the affinity as logit is not the way to discrimitate between likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figs/PGM.png \"Title\")\n",
    "*PGM of model. We use the notation of http://www.mbmlbook.com that specifices the pgm as a bipartite graph where the squares explicitely denotes the distribution* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = \"\"\" data {\n",
    "    int num_movies;             // number of data items\n",
    "    int num_traits;\n",
    "    int num_users;  \n",
    "    \n",
    "    int num_likes;\n",
    "\n",
    "    \n",
    "    int likes_obs[num_likes];\n",
    "    int userId_obs[num_likes];\n",
    "    int movieId_obs[num_likes];\n",
    "    \n",
    "    int num_missing;\n",
    "    int userId_missing [num_missing];\n",
    "    int movieId_missing [num_missing];\n",
    "    \n",
    "}\n",
    "parameters {\n",
    "    matrix[num_movies,num_traits] trait;\n",
    "    matrix[num_users ,num_traits] preference;\n",
    "    vector[num_movies] trait_bias;\n",
    "    vector[num_users] preference_bias;\n",
    "    \n",
    "} \n",
    "\n",
    "model {\n",
    "    real affinity;\n",
    "    //fix symmetries\n",
    "    /*for(i in 1:num_traits){\n",
    "        for( j in 1:num_traits){\n",
    "            if(i == j){\n",
    "                trait[i,j] ~ normal(1,0.0001);\n",
    "            }\n",
    "            else{\n",
    "                trait[i,j] ~ normal(0,0.0001);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    */\n",
    "    for (n in 1:num_likes){\n",
    "        affinity = 0;\n",
    "        trait_bias[movieId_obs[n]] ~ normal(0,10);\n",
    "        preference_bias[userId_obs[n]] ~ normal(0,10);\n",
    "        for (t in 1:num_traits){\n",
    "            preference[userId_obs[n], t] ~ normal(0,10);\n",
    "            //if (movieId_obs[n] > num_traits){\n",
    "                trait[movieId_obs[n], t] ~ normal(0,10);\n",
    "            //}\n",
    "            affinity += trait[movieId_obs[n], t]*preference[userId_obs[n], t];\n",
    "            \n",
    "        }\n",
    "        affinity += trait_bias[movieId_obs[n]] + preference_bias[userId_obs[n]];\n",
    "        \n",
    "        likes_obs[n] ~ bernoulli_logit(affinity);\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    real<lower=0, upper=1> predictions[num_missing];\n",
    "    \n",
    "    for(i in 1:num_missing){\n",
    "        real affinity = 0;\n",
    "        for (t in 1:num_traits){\n",
    "            affinity += trait[movieId_missing[i], t] * preference[userId_missing[i], t];\n",
    "        }\n",
    "        affinity += trait_bias[movieId_missing[i]] + preference_bias[userId_missing[i]];\n",
    "        predictions[i] = bernoulli_logit_rng(affinity);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Stan model object\n",
    "sm = pystan.StanModel(model_code = model_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data set\n",
    "Generate a simple data containing two groups (p1,p2) of people and to groups of movies (m1,m2).\n",
    "* p1 likes all movies in m1 but dislikes all movies in m2.\n",
    "* p2 likes all movies in m2 but dislikes all movies in m1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_fake, val_set_fake = generate_fake_data(val_size=0.3, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake, num_users_fake, num_movies_fake = generate_data_dict(train_set_fake, val_set_fake,n_traits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#sampling takes forever here, but VB seems to work really well\n",
    "#fit = sm.sampling(data=data, iter=100, algorithm=\"NUTS\", chains=1, seed=seed, verbose=True)\n",
    "fit_fake = sm.vb(data=data_fake,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fake, probabilities_fake = pystan_utils.vb_extract_predictions(fit_fake)\n",
    "get_precision(predictions_fake,val_set_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is 1.0 which shows that we can predict the two groups accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences_fake=pystan_utils.vb_extract_variable(fit_fake, 'preference[', var_type='matrix', dims=[num_users_fake,2])\n",
    "traits_fake=pystan_utils.vb_extract_variable(fit_fake, 'trait[', var_type='matrix', dims=[num_movies_fake,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(preferences_fake[:,0], preferences_fake[:,1],label='preference')\n",
    "plt.scatter(traits_fake[:,0], traits_fake[:,1], label='traits')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preference and traits are nicely separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "As a baseline implimentation we use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Damped Baseline with beta=0...\", end=\"\")\n",
    "model = DampedUserMovieBaselineModel(damping_factor=0)\n",
    "#validator = PerformanceOverTimeValidator(model, n_year_period=2)\n",
    "#years_4, errs_4 = validator.validate(ratings[['userId', 'movieId']], ratings['rating'], ratings['year'])\n",
    "#print(\"Done!\")\n",
    "model.fit(train_set[['userId', 'movieId']], train_set['rating'])\n",
    "baseline_prediction = model.predict(val_set[['userId', 'movieId']])\n",
    "get_NDCG(baseline_prediction, val_set, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_users, num_movies = generate_data_dict(train_set, val_set,n_traits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#sampling takes forever here, but VB seems to work really well\n",
    "#fit = sm.sampling(data=data, iter=100, algorithm=\"NUTS\", chains=1, seed=seed, verbose=True)\n",
    "fit = sm.vb(data=data,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, probabilities = pystan_utils.vb_extract_predictions(fit)\n",
    "get_precision(predictions,val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is larger than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_NDCG(probabilities,val_set, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show preferences\n",
    "Here the latent traits and preferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences=pystan_utils.vb_extract_variable(fit, 'preference[', var_type='matrix', dims=[num_users,2])\n",
    "traits=pystan_utils.vb_extract_variable(fit, 'trait[', var_type='matrix', dims=[num_movies,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(preferences[:,0], preferences[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(traits[:,0], traits[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some extreme values of trait0. We would expect to see that the trait is discriminating between films using a latent trait of the film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_extreme = 10\n",
    "sorted_trait_0_ids = np.argsort(traits[:,0])\n",
    "lowest_ids = sorted_trait_0_ids[:n_extreme]+1\n",
    "highest_ids = sorted_trait_0_ids[-n_extreme:]+1\n",
    "lowest_movie_ids = [ id_movie_dict[lowest_id] for lowest_id in lowest_ids]\n",
    "highest_movie_ids = [ id_movie_dict[highest_id] for highest_id in highest_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies.movieId.isin(lowest_movie_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies.movieId.isin(highest_movie_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspection of the low/high scoring, we cannot really see any latent trait that is used for discriminating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar films should have similiar trait values. Therefore LOTR movies are inspected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_lotr = [movie_id_dict[movie]-1 for movie in  movies[movies.title.str.contains('Lord of the Rings')].movieId]\n",
    "plt.figure()\n",
    "plt.scatter(traits[:,0], traits[:,1])\n",
    "plt.scatter(traits[ids_lotr][:,0],traits[ids_lotr][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the movies for which the porsterior of the traits has low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_low_variance_movies(fit,movies,num_movies,id_movie_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting traits and biases for some movies to see what dominates what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_var,means_var, stds_var, names_var=pystan_utils.vb_extract(fit)\n",
    "plt.figure()\n",
    "for i in range(1,4):\n",
    "    sn.kdeplot(samples_var[f'trait_bias[{i}]'], color='b')\n",
    "    sn.kdeplot(samples_var[f'trait[{i},1]'], color='r')\n",
    "    sn.kdeplot(samples_var[f'trait[{i},2]'], color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that in some cases the bias is close to zero and sometimes not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition_genre = \"\"\" data {\n",
    "    int num_movies;             // number of data items\n",
    "    int num_traits;\n",
    "    int num_users;  \n",
    "    \n",
    "    int num_likes;\n",
    "\n",
    "    \n",
    "    int likes_obs[num_likes];\n",
    "    int userId_obs[num_likes];\n",
    "    int movieId_obs[num_likes];\n",
    "    \n",
    "    int num_missing;\n",
    "    int userId_missing [num_missing];\n",
    "    int movieId_missing [num_missing];\n",
    "    \n",
    "    matrix[num_likes,20] genre;\n",
    "    \n",
    "}\n",
    "parameters {\n",
    "    matrix[num_movies,num_traits] trait;\n",
    "    matrix[num_users ,num_traits] preference;\n",
    "    vector[num_movies] trait_bias;\n",
    "    vector[num_users] preference_bias;\n",
    "    matrix[20,num_traits] genre_weight;\n",
    "    vector[num_traits] genre_bias;\n",
    "    \n",
    "} \n",
    "\n",
    "model {\n",
    "    real affinity;\n",
    "    row_vector[num_traits] traitMean;\n",
    "    \n",
    "    genre_bias ~ normal(0,2);\n",
    "    \n",
    "    for (t in 1:num_traits){\n",
    "        genre_weight[t] ~ normal(0,2);\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for (n in 1:num_likes){\n",
    "        affinity = 0;\n",
    "        \n",
    "        traitMean = genre[n]*genre_weight;\n",
    "        \n",
    "        trait_bias[movieId_obs[n]] ~ normal(0,10);\n",
    "        preference_bias[userId_obs[n]] ~ normal(0,10);\n",
    "        \n",
    "        \n",
    "        for (t in 1:num_traits){\n",
    "            \n",
    "            preference[userId_obs[n], t] ~ normal(0,10);\n",
    "            trait[movieId_obs[n], t] ~ normal(traitMean[t] ,10);\n",
    "\n",
    "            affinity += trait[movieId_obs[n], t]*preference[userId_obs[n], t];\n",
    "            \n",
    "        }\n",
    "        affinity += trait_bias[movieId_obs[n]] + preference_bias[userId_obs[n]];\n",
    "        \n",
    "        likes_obs[n] ~ bernoulli_logit(affinity);\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    real<lower=0, upper=1> predictions[num_missing];\n",
    "    \n",
    "    for(i in 1:num_missing){\n",
    "        real affinity = 0;\n",
    "        for (t in 1:num_traits){\n",
    "            affinity += trait[movieId_missing[i], t] * preference[userId_missing[i], t];\n",
    "        }\n",
    "        affinity += trait_bias[movieId_missing[i]] + preference_bias[userId_missing[i]];\n",
    "        predictions[i] = bernoulli_logit_rng(affinity);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Stan model object\n",
    "sm_genre = pystan.StanModel(model_code = model_definition_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_users, num_movies = generate_data_dict(train_set, val_set,n_traits=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#sampling takes forever here, but VB seems to work really well\n",
    "#fit = sm.sampling(data=data, iter=100, algorithm=\"NUTS\", chains=1, seed=seed, verbose=True)\n",
    "fit_genre = sm_genre.vb(data=data,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_genre, probabilities_genre = pystan_utils.vb_extract_predictions(fit_genre)\n",
    "get_precision(predictions_genre,val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is larger than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_NDCG(probabilities_genre,val_set, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_var,means_var, stds_var, names_var=pystan_utils.vb_extract(fit_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,20+1):\n",
    "    sn.kdeplot(samples_var[f'genre_weight[{i},1]'], color='b')\n",
    "    sn.kdeplot(samples_var[f'genre_weight[{i},2]'], color='r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Expectation Propagation vs. Variational Bayes (10% difference in precision)?*\n",
    "* *Does these values make sense (low ELBO)?*\n",
    "```\n",
    "Begin stochastic gradient ascent.\n",
    "  iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \n",
    "   100     -3124403.404             1.000            1.000\n",
    "   200     -1131926.528             1.380            1.760\n",
    "   300     -1068337.903             0.940            1.000\n",
    "   400     -1037532.838             0.712            1.000\n",
    "   500      -982924.262             0.581            0.060\n",
    "   600     -1024784.262             0.491            0.060\n",
    "   700      -967588.749             0.429            0.059\n",
    "   800      -966833.896             0.376            0.059\n",
    "   900      -954641.153             0.335            0.056\n",
    "  1000      -932961.912             0.304            0.056\n",
    "  1100      -933640.840             0.204            0.041\n",
    "  1200      -944417.031             0.029            0.030\n",
    "  1300      -933809.949             0.025            0.023\n",
    "  1400      -922032.712             0.023            0.013\n",
    "  1500      -929080.979             0.018            0.013\n",
    "  1600      -923651.819             0.015            0.011\n",
    "  1700      -926248.001             0.009            0.011   MEAN ELBO CONVERGED\n",
    " ```\n",
    "* *Should we care about symmetries?* \n",
    "```samples_dict, means_dict, var_names = pystan_utils.vb_extract(fit2)\n",
    "pystan_utils.plot_kde(samples_dict['trait[1,1]'])\n",
    "```\n",
    "* *Why do we not get the same values for lotr in the plots*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling stars\n",
    "\n",
    "To model star ratings, instead of likes we need to change our model from a binary classification model, to a multivariate one.\n",
    "\n",
    "This can be done by simply replacing the bernoulli distribution with a categorical to generate outputs between 1-10.\n",
    "Now we however need to model the parameters of the categorical distribution. Our affinity variable is just a number, so a naive way would be to define parameters $\\beta$ as \n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta}_c = [\\beta_1, \\beta_2, \\dots , \\beta_c] = w^T\\text{affinity}\n",
    "$$\n",
    "\n",
    "where $w\\in \\mathbb{R}^C$\n",
    "\n",
    "\n",
    "$$\n",
    "star_{u,m} \\sim categorical(star_{u,m} |\\text{softmax}( \\mathbf{w}^T\\text{affinity}_{u,m}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = \"\"\" data {\n",
    "    int num_movies;             // number of data items\n",
    "    int num_traits;\n",
    "    int num_users;  \n",
    "    \n",
    "    int num_likes;\n",
    "\n",
    "    \n",
    "    int stars_obs[num_likes];\n",
    "    int userId_obs[num_likes];\n",
    "    int movieId_obs[num_likes];\n",
    "    \n",
    "    int num_missing;\n",
    "    int userId_missing [num_missing];\n",
    "    int movieId_missing [num_missing];\n",
    "    \n",
    "}\n",
    "parameters {\n",
    "    matrix[num_movies,num_traits] trait;\n",
    "    matrix[num_users ,num_traits] preference;\n",
    "    vector[num_movies] trait_bias;\n",
    "    vector[num_users] preference_bias;\n",
    "    vector[10] w;\n",
    "    \n",
    "} \n",
    "\n",
    "model {\n",
    "    real affinity;\n",
    "    w ~normal(0,10);\n",
    "    for (n in 1:num_likes){\n",
    "        affinity = 0;\n",
    "        trait_bias[movieId_obs[n]] ~ normal(0,10);\n",
    "        preference_bias[userId_obs[n]] ~ normal(0,10);\n",
    "        for (t in 1:num_traits){\n",
    "            preference[userId_obs[n], t] ~ normal(0,10);\n",
    " \n",
    "            affinity += trait[movieId_obs[n], t]*preference[userId_obs[n], t];\n",
    "            \n",
    "        }\n",
    "        affinity += trait_bias[movieId_obs[n]] + preference_bias[userId_obs[n]];\n",
    "        \n",
    "        stars_obs[n] ~ categorical_logit(affinity * w);\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    int predictions[num_missing];\n",
    "    \n",
    "    for(i in 1:num_missing){\n",
    "        real affinity = 0;\n",
    "        for (t in 1:num_traits){\n",
    "            affinity += trait[movieId_missing[i], t] * preference[userId_missing[i], t];\n",
    "        }\n",
    "        affinity += trait_bias[movieId_missing[i]] + preference_bias[userId_missing[i]];\n",
    "        predictions[i] = categorical_logit_rng(affinity*w);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create Stan model object\n",
    "sm_stars = pystan.StanModel(model_code = model_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_users, num_movies = generate_data_dict(train_set, val_set,n_traits=2, stars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#sampling takes forever here, but VB seems to work really well\n",
    "#fit = sm.sampling(data=data, iter=100, algorithm=\"NUTS\", chains=1, seed=seed, verbose=True)\n",
    "fit_stars = sm_stars.vb(data=data,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_stars, probabilities_stars = pystan_utils.vb_extract_predictions(fit_stars, multi_class=True)\n",
    "get_precision(predictions_stars,val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pystan_utils.vb_extract_variable(fit_stars, 'w', 'vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Doesn't work for multiclass output at the momement\n",
    "get_NDCG(probabilities_stars,val_set, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
